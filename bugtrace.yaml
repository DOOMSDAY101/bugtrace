llm:
  provider: ollama
  model: llama3.2:3b
  temperature: 0.2
paths:
  project_root: .
  ignore:
    - node_modules
    - venv
    - .env
    - .git
    - .bugtrace
    - "*.pyc"
    - "__pycache__"
    - bugtrace.egg-info
  logs:
    - logs/
rag:
  chunk_size: 1000
  chunk_overlap: 200
  top_k: 6
  store: chroma
tools:
  code_search: true
  log_search: true
  config_check: true
analysis:
  max_steps: 5
  reasoning_style: concise
